import { tool } from "@anthropic-ai/claude-agent-sdk";
import { z } from "zod";
import { retryWithBackoff } from "../utils/retry.js";
import { urlValidator } from "../utils/validation.js";
import type { BodegonConfig, CapturedImage } from "../agents/types.js";

/**
 * Herramienta MCP para scraping de im√°genes de productos
 */
export function createImageScraperTool(config: BodegonConfig) {
  return tool(
    "scrapeProductImages",
    "Extrae im√°genes de productos de URLs de e-commerce con reintentos autom√°ticos y validaci√≥n",
    {
      url: z.string().url("La URL debe ser v√°lida"),
      maxImages: z.number().int().min(1).max(10).default(3).describe("N√∫mero m√°ximo de im√°genes a capturar"),
      imageQuality: z.enum(['high', 'medium', 'low']).default('high').describe("Calidad de las im√°genes"),
      outputDirectory: z.string().optional().describe("Directorio de salida personalizado"),
    },
    async (args, _extra) => {
      try {
        // Ensure required parameters are present
        if (!args.url) {
          return {
            content: [{
              type: "text",
              text: "Error: url parameter is required"
            }],
            isError: true
          };
        }

        // Validar URL
        if (!urlValidator.isValidUrl(args.url)) {
          throw new Error(`URL inv√°lida: ${args.url}`);
        }

        if (!urlValidator.isAllowedDomain(args.url, config.scraping.allowedDomains)) {
          throw new Error(`Dominio no permitido. Dominios permitidos: ${config.scraping.allowedDomains.join(', ')}`);
        }

        const result = await retryWithBackoff(
          () => scrapeImages({
            url: args.url,
            maxImages: args.maxImages || 3,
            imageQuality: args.imageQuality || 'high',
            outputDirectory: args.outputDirectory
          }, config),
          config.retry,
          `scraping-${args.url}`
        );

        return {
          content: [{
            type: "text",
            text: `‚úÖ Se capturaron ${result.length} im√°genes de ${args.url}:\n${result.map(img => `  üì∏ ${img.filename}`).join('\n')}`
          }]
        };
      } catch (error) {
        const errorMessage = error instanceof Error ? error.message : 'Error desconocido';
        return {
          content: [{
            type: "text",
            text: `‚ùå Error al capturar im√°genes de ${args.url}: ${errorMessage}`
          }],
          isError: true
        };
      }
    }
  );
}

/**
 * Funci√≥n principal de scraping con implementaci√≥n mock para desarrollo
 */
async function scrapeImages(
  args: { url: string; maxImages: number; imageQuality?: string; outputDirectory?: string },
  config: BodegonConfig
): Promise<CapturedImage[]> {
  const outputDir = args.outputDirectory || config.composition.outputDirectory;

  // Asegurar que el directorio de salida exista
  await ensureDirectory(outputDir);

  // Simular el proceso de scraping
  console.log(`üåê Iniciando scraping de: ${args.url}`);

  // En una implementaci√≥n real, aqu√≠ usar√≠amos Playwright MCP
  // Por ahora, simulamos el proceso con datos de ejemplo
  const capturedImages: CapturedImage[] = [];

  const imageCount = Math.min(args.maxImages, config.scraping.maxImages);

  for (let i = 1; i <= imageCount; i++) {
    const timestamp = new Date();
    const filename = `product-${Date.now()}-${i}.png`;
    const path = `${outputDir}/${filename}`;

    // Simular captura de imagen
    await simulateImageCapture(filename, path, args.imageQuality || config.scraping.imageQuality);

    const capturedImage: CapturedImage = {
      url: `${args.url}/image-${i}`,
      filename,
      path,
      size: estimateImageSize(args.imageQuality || config.scraping.imageQuality),
      format: config.scraping.screenshotFormat as 'png' | 'jpeg',
      timestamp,
      sourceUrl: args.url,
      productTitle: extractProductTitle(args.url)
    };

    capturedImages.push(capturedImage);
    console.log(`  üì∏ Capturada imagen ${i}/${imageCount}: ${filename}`);

    // Simular delay entre capturas
    await new Promise(resolve => setTimeout(resolve, 500));
  }

  console.log(`‚úÖ Completado: ${capturedImages.length} im√°genes capturadas`);
  return capturedImages;
}

/**
 * Simula la captura de una imagen (implementaci√≥n mock)
 */
async function simulateImageCapture(_filename: string, path: string, quality: string): Promise<void> {
  // Simular creaci√≥n de archivo de imagen
  const _mockImageData = generateMockImageData(quality);

  // En una implementaci√≥n real, aqu√≠ usar√≠amos fs para guardar la imagen
  console.log(`üíæ Guardando imagen: ${path} (${quality} quality)`);

  // Simular tiempo de guardado
  await new Promise(resolve => setTimeout(resolve, 200));
}

/**
 * Genera datos de imagen mock basados en la calidad
 */
function generateMockImageData(quality: string): Buffer {
  const sizes = {
    high: 1024 * 768 * 3, // ~2.3MB
    medium: 800 * 600 * 3, // ~1.4MB
    low: 640 * 480 * 3 // ~900KB
  };

  const size = sizes[quality as keyof typeof sizes] || sizes.medium;
  return Buffer.alloc(size, 'mock-image-data');
}

/**
 * Estima el tama√±o de archivo basado en la calidad
 */
function estimateImageSize(quality: string): number {
  const sizes = {
    high: 2_300_000, // ~2.3MB
    medium: 1_400_000, // ~1.4MB
    low: 900_000 // ~900KB
  };

  return sizes[quality as keyof typeof sizes] || sizes.medium;
}

/**
 * Extrae un t√≠tulo de producto simulado de la URL
 */
function extractProductTitle(url: string): string {
  const domain = urlValidator.extractDomain(url);

  // Simular diferentes tipos de productos basados en el dominio
  const productTemplates = {
    'exito.com': ['Consola Nintendo Switch', 'Laptop Gamer ASUS', 'Smartphone Samsung Galaxy', 'Auriculares Bluetooth', 'Tablet iPad'],
    'falabella.com': ['Televisor LG 4K', 'Refrigerador Samsung', 'Lavadora Whirlpool', 'Microondas Oster', 'Cafetera Nespresso'],
    'linio.com': ['C√°mara Canon', 'Drone DJI', 'Smartwatch Apple', 'Altavoz JBL', 'Teclado Mec√°nico'],
    'mercadolibre.com': ['PlayStation 5', 'Xbox Series X', 'Nintendo Switch OLED', 'Steam Deck', 'Gaming PC'],
  };

  const domainProducts = productTemplates[domain as keyof typeof productTemplates] || productTemplates['mercadolibre.com'];
  return domainProducts[Math.floor(Math.random() * domainProducts.length)];
}

/**
 * Asegura que un directorio exista
 */
async function ensureDirectory(path: string): Promise<void> {
  // En una implementaci√≥n real, usar√≠amos fs-extra.ensureDir
  console.log(`üìÅ Verificando directorio: ${path}`);
}

/**
 * Herramienta para validaci√≥n de URLs antes del scraping
 */
export function createUrlValidationTool(config: BodegonConfig) {
  return tool(
    "validateScrapingUrl",
    "Valida si una URL es adecuada para scraping de im√°genes",
    {
      url: z.string().url("La URL debe ser v√°lida"),
    },
    async (args, _extra) => {
      const validation = {
        isValid: true,
        isAllowed: true,
        domain: urlValidator.extractDomain(args.url || ''),
        issues: [] as string[],
        recommendations: [] as string[]
      };

      // Validar formato de URL
      if (!urlValidator.isValidUrl(args.url)) {
        validation.isValid = false;
        validation.issues.push("Formato de URL inv√°lido");
      }

      // Validar dominio permitido
      if (!urlValidator.isAllowedDomain(args.url, config.scraping.allowedDomains)) {
        validation.isAllowed = false;
        validation.issues.push(`Dominio no permitido: ${validation.domain}`);
        validation.recommendations.push(`Dominios permitidos: ${config.scraping.allowedDomains.join(', ')}`);
      }

      // Recomendaciones
      if (!args.url.includes('/product') && !args.url.includes('/item') && !args.url.includes('/p/')) {
        validation.recommendations.push("Considera usar URLs de productos espec√≠ficos para mejores resultados");
      }

      const status = validation.isValid && validation.isAllowed ? '‚úÖ' : '‚ùå';

      return {
        content: [{
          type: "text",
          text: `${status} Validaci√≥n de URL: ${args.url}\n` +
                `üìå Dominio: ${validation.domain}\n` +
                `‚úÖ Formato v√°lido: ${validation.isValid ? 'S√≠' : 'No'}\n` +
                `üîê Dominio permitido: ${validation.isAllowed ? 'S√≠' : 'No'}\n` +
                (validation.issues.length > 0 ? `‚ö†Ô∏è  Problemas: ${validation.issues.join(', ')}\n` : '') +
                (validation.recommendations.length > 0 ? `üí° Recomendaciones: ${validation.recommendations.join(', ')}` : '')
        }]
      };
    }
  );
}

/**
 * Herramienta para obtener estad√≠sticas de scraping
 */
export function createScrapingStatsTool() {
  return tool(
    "getScrapingStats",
    "Obtiene estad√≠sticas del proceso de scraping",
    {
      outputPath: z.string().optional().describe("Path al directorio para analizar"),
    },
    async (args, _extra) => {
      // Simular estad√≠sticas
      const stats = {
        totalImages: 0,
        totalSize: 0,
        averageSize: 0,
        formatDistribution: {
          png: 0,
          jpeg: 0
        },
        qualityDistribution: {
          high: 0,
          medium: 0,
          low: 0
        },
        recentActivity: [] as string[]
      };

      // En una implementaci√≥n real, leer√≠amos el sistema de archivos
      console.log(`üìä Analizando estad√≠sticas de scraping${args.outputPath ? ` en ${args.outputPath}` : ''}`);

      return {
        content: [{
          type: "text",
          text: `üìä Estad√≠sticas de Scraping:\n` +
                `üì∏ Total de im√°genes: ${stats.totalImages}\n` +
                `üíæ Tama√±o total: ${(stats.totalSize / 1024 / 1024).toFixed(2)} MB\n` +
                `üìè Tama√±o promedio: ${(stats.averageSize / 1024).toFixed(2)} KB\n` +
                `üñºÔ∏è  Formatos: PNG (${stats.formatDistribution.png}), JPEG (${stats.formatDistribution.jpeg})\n` +
                `üé® Calidades: Alta (${stats.qualityDistribution.high}), Media (${stats.qualityDistribution.medium}), Baja (${stats.qualityDistribution.low})`
        }]
      };
    }
  );
}

// Exportar todas las herramientas
export function createAllScrapingTools(config: BodegonConfig) {
  return [
    createImageScraperTool(config),
    createUrlValidationTool(config),
    createScrapingStatsTool()
  ];
}